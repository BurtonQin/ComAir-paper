\section{Introduction}
\label{sec:intro}

\subsection{Motivation}
\label{sec:motiv}

Performance problems~\footnote{We will use performance problems and performance bugs exchangeably, 
following previous works in this area~\cite{SongOOPSLA2014,ldoctor}} 
are one type of software implementation mistakes
and can cause inefficient execution~\cite{PerfBug,perf.fse10,SongOOPSLA2014,ldoctor,Alabama}. 
Performance problems cannot be optimized away by state-of-the-art compiler optimizations.
Due to the complexity of modern software and rapidly changing workloads, 
performance problems widely exist in production-run software, 
annoying end users and wasting energy in the field~\cite{PerfBug,SongOOPSLA2014,ldoctor}. 
Many highly-publicized failures have already been caused by performance problems, 
such as making a website costing millions of dollars useless~\cite{ACA-health}.
Combating performance problems is urgent. 

Many performance problems are caused by algorithmic inefficiency, 
such as implementing a linear algorithm in a $O(N^2)$ way.
We refer these performance problems as complexity problems in our paper.
Our empirical study on a representative performance-bug 
benchmark set~\cite{PerfBug,SongOOPSLA2014} shows that 
nearly half user-perceived performance problems are complexity problems. 
Complexity problems are usually ranked high in developers' priority list. 
For example, Mozilla developers will immediately try to fix complexity bugs degrading exponentially~\cite{mozilla35294}. 
Addressing complexity problems is an important aspect to fight performance bugs. 


Algorithmic profiling~\cite{Aprof1,Aprof2,AlgoProf} collects profiles from multiple 
runs of the same program and attributes complexity to each code construct, like a loop or a function,
in the form of a \textit{cost} function of \textit{input} size. 
Algorithmic profiling can be used detect previously unknown complexity problems and 
diagnose performance failures caused by complexity problems. 

Conducting algorithmic profiling is challenging. 
Effective techniques need to satisfy the following three requirements.
First, \textit{coverage}. Complexity problems are caused by a large variety of root causes. 
A code construct may take inputs and consume computation resources in various types.
A good technique must cover a large portion of various root causes, inputs and costs.
Second, \textit{accuracy}. Algorithmic profiling needs to accurately identify 
how cost scales as input size changes.
Given an analyzed code construct,
it is desired to conduct algorithmic profiling under the context of the whole program's execution. 
Otherwise, missing how the code construct cooperate with other parts will lead to inaccurate results. 
Third, \textit{performance}. 
For in-house testing, lowering runtime overhead can save testing time 
and allow developers to run more tests under a given time budget. 
Developers' misunderstanding of real-world workloads 
is the most common reason why performance problems are introduced~\cite{PerfBug}. 
Production-run algorithmic profiling can help developers 
understand how their programs scale under real-world workloads.
However, production-run techniques require no observable slowdown from end users. 

Existing techniques do not satisfy the above three conditions and 
cannot conduct algorithmic profiling effectively. 
Traditional profilers are most widely used tools to 
diagnose performance failure~\cite{gprof,oprofile}. 
Traditional profilers can only measure how much time spent in each code construct during one single run, 
while failing to connect information from multiple runs 
and failing to provide any indication about how execution time scales.
Therefore, traditional profilers fail in both coverage and accuracy.  
To understand asymptotic complexity for a code construct,
experimental algorithmics~\cite{expalg1,expalg2,expalg3} requires developers to 
extract the code construct and analyze it out of the original program. 
Although useful,
experimental algorithmics fail to consider how the code 
construct interact with the whole program and does not provide desired accuracy. 
Recent techniques on algorithmic profiling~\cite{Aprof1,Aprof2,AlgoProf} will incur more than 30X runtime overhead.
They cannot provide desired performance and are far from being deployed in production runs. 


\begin{figure}
\centering
\lstset{basicstyle=\ttfamily\fontsize{7}{8}\selectfont,
     morekeywords={+},keepspaces=true,numbers=left}
  \mbox{\lstinputlisting[mathescape,boxpos=t]{figure/mysql27287.c}}
\caption{A MySQL performance problem in polynomial complexity. 
\footnotesize{(During performance failure runs, 
   the total loop iterations scale polynomially in the size of \texttt{items}.)}}
\vspace{-0.05in}
\label{fig:mysql27287}
\vspace{-0.05in}
\end{figure}

One MySQL complexity problem is shown in Figure~\ref{fig:mysql27287}.
The loop searches parent \texttt{XML\_NODE} for function parameter \texttt{nitems}, 
which presents an array index for another \texttt{XML\_NODE}.
All \texttt{XML\_NODE}s are maintained in array \texttt{items}. 
The loop searches by iterating array \texttt{items} 
backward from the input and looking for the first \texttt{XML\_NODE} 
whose level is one less than the input.
This piece of codes looks innocent. 
However, there is an outer loop not shown in the figure.
The outer loop will keep calling \texttt{xml\_parent\_tag} by using 
the next sibling of the previous \texttt{XML\_NODE}, 
which is in $O(N^2)$ complexity in terms of the number of children of a parent \texttt{XML\_NODE}. 
Developers may think using an implementation in $O(N^2)$ complexity is fine, 
since a \texttt{XML\_NODE} usually does not have too many children.
However, during performance failure runs, 
one \texttt{XML\_NODE} contains tens of thousands of children, 
and this leads significant showdown perceived by end users. 

To fix this bug, developers add an extra field to each \texttt{XML\_NODE} to save its parent, 
and this field is initialized when a \texttt{XML\_NODE} is created. 
After applying this patch, codes shown in Figure~\ref{fig:mysql27287} are completely removed.
This takes developers around 5 months to figure 
out~\footnote{We count the time from when developers confirmed this is performance bug 
to when the patch was submitted}. 
In-house algorithmic profiling can warn developers the $O(N^2)$ complexity, 
potentially obscured by the fact that inner and outer loops are not in the same function. 
Production-run algorithmic profiling can provide information about workload and complexity,
which are invaluable to diagnose the performance failures caused by this complexity problem. 




\subsection{Contributions}
\label{sec:con}


Why we need algorithmic profiling? 

Why we need to push complexity profiling to production runs? 
a. Understanding real-world workload
b. Identifying performance bugs caused by API-misuse

The state of the art cannot be applied to production runs. 

To better understand real-world complexity problems,
we conduct an empirical study on complexity bugs 
in a representative performance bug benchmark suite~\cite{PerfBug,SongOOPSLA2014}.
To the best of our knowledge, our work is the first study focusing on complexity problems.
We divide studied complexity problems into different complexity categories.    
We study root causes, how user-perceived performance impact is generated, 
and fix strategies for each category.
We also investigate the reporting and diagnosis process for complexity problems. 
Our findings and implications can motivate future research on complexity problems. 
They already guide our design point selection when exploring in-house algorithmic profiling 
and inspire our approximate algorithms when building production-run techniques. 

After carefully studying the design points for in-house algorithmic profiling,
we started to build our in-house technique 
based on an existing algorithm~\cite{Aprof1,Aprof2} leveraging 
Read Memory Size (RMS) as input metric. 
We design 4 compiler and runtime optimizations 
to reduce the runtime overhead during algorithmic profiling.
Our optimizations can reduce instrumentation sites 
and can also accelerate instrumented hook functions. 
There are two scenarios where the existing algorithm fails to profile, 
which are recursive functions scaling as the value of input parameters 
and code constructs taking inputs from I/O.
We enhance the existing algorithm to address these two limitations. 




Specifically, we make the following contributions:

\begin{itemize}

\item First, we conduct the first empirical study on real-world complexity problems. 
We have several important findings and implications, including
1) around three fourths of studied complexity problems are 
caused by repeated executions of buggy code constructs,
such as loops or recursive functions;
2) for most complexity problems, 
users describe how to change input size to observe the scaling problem during reporting;
and 3) complexity problems usually take longer time to diagnose and fix, 
and more effective tool supports are needed.  

\item Second, we optimize and enhance an in-house algorithmic profiling algorithm.
Our proposed optimizations 
can reduce instrumentation sites and accelerate hook functions.
We augment the algorithm to handle two scenarios, 
where the existing version cannot profile correctly. 





\item Third, we propose two approximate algorithms to conduct algorithmic profiling in production runs. 

\end{itemize}

