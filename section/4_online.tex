\newpage
\section{Production-Run Algorithmic Profiling}
\label{sec:online}

In this section, we discuss our design and 
implementation for the production-run version of \Tool. 
For production-run usage, profiles are collected from the user side.
It is very important to keep a low runtime overhead, since
end users cannot tolerate any observable performance slowdown.
To achieve this requirement,
our design follows several principles. 

First, \textit{study-guided}. 
We design the production-run version of \Tool
under the guidance of the empirical study in Section~\ref{sec:study}.
We focus on the majority of complexity problems, 
caused either by repeated execution of a loop ($N^k$)
or a recursive function ($2^N$).
We focus on common types of data structures, which are array and linked list.

Second, \textit{focused checking}.
To apply the production-run version of \Tool, 
we expect developers to specify a suspicious loop or a suspicious recursive function
to be monitored. 
\Tool will automatically instrument the specified code construct 
in order to collect runtime information from the user side in a low overhead. 
The profiling results can help developers better understand the code construct' 
complexity and processed workload in the real world.
\Tool can be used together with performance failure 
diagnosis tools~\cite{SongOOPSLA2014} 
or traditional profilers,
focusing on suspicious code constructs leading 
to user-perceived performance failures.

Third, \textit{sampling}.
Instead of recording all dynamic information, 
we apply sampling and only record part of the information. 
We infer information for the whole execution based on collected samples. 
Sampling can effectively lower the runtime overhead. 


\subsection{Technical Design}
As we discussed in Section~\ref{sec:study}, 
the majority of studied complexity problems are caused 
by repeated executions of a loop or a recursive function. 
Previous works show that sampling code constructs that are executed 
multiple times in one program run can lower runtime overhead, 
while still being able to collect enough runtime information 
without hurting diagnosis latency~\cite{SongOOPSLA2014,ldoctor}. 
Inspired by our study and the earlier works, 
we apply sampling to efficiently profile loops 
and recursive functions with multiple executions. 

\noindent\textbf{Input Metric}
If the specified code construct is an array-processing loop 
or a linked-list-processing loop,
we will use DSS as input metric. 
Otherwise, we will use RMS+ as input metric. 

As we discussed in Section~\ref{sec:inhouse}, 
there are, top-down and bottom-up, 
two methods to analyze RMS+ and DSS records collected 
for multiple dynamic instances of a code construct in one program run. 
We leverage the top-down method for the in-house version of \Tool. 
However, the top-down method is not suitable to be combined with sampling. 
The reason is explained as follows.
Assume we have a code construct \texttt{c} to monitor. 
It is inside a loop \texttt{l} and is executed multiple times in one program run.
Dynamic instances of \texttt{l} are much less than 
dynamic instances of \texttt{c}.
If we apply the top-down method, 
we need to sample dynamic instances of \texttt{l}, 
and it is very likely that we will miss these instances. 
For each dynamic instance of \texttt{l}, 
it will conduct more computation, 
compared with an instance of \texttt{c}.
It will incur more overhead to collect 
information for dynamic instances of \texttt{l}.
Therefore, we apply the bottom-up method 
in the production-run version of \Tool.
We sample instances of \texttt{c} to record 
distinct memory cells contributing RMS+ 
or distinct elements in an array or a linked list.
We use the sampled information to infer RMS+ 
or DSS for all instances of \texttt{c} in one program run.


How to do the sampling is similar to previous works in statistical 
debugging~\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}.
We make a cloned version of the monitored code construct.
We instrument the cloned version to record information for RMS+ or DSS. 
We dump the recorded information to log 
whenever the cloned version finishes its execution. 
We add extra delimiters to log to differ information collected from different instances.
Before each execution of the monitored code construct, 
we choose between the cloned version and the original version. 
How many times the cloned version is executed 
depends on a tunable sampling rate. 
To make the choice between the two versions,
we add a global counter to the monitored program. 
If the counter value is larger than $0$, 
we choose the original version and decrease the counter value by $1$.
If the counter value is equal to $0$,
we choose the cloned version and reset the counter value to 
a random number, 
whose expectation is equal to the inverse of the sampling rate.  


We leverage mark-and-recapture method~\citep{mark-recapture} to 
estimate RMS+ or DSS for all dynamic instances of a code construct 
based on collected samples. 
Mark-and-recapture is a commonly used statistical method 
to estimate the size of animal population. 
By using this method, a portion of animals is captured, marked, and released. 
Then, another portion is captured.
The size of the whole animal population is estimated 
based on the ratio of marked animals in the second captured portion.  


Each sample of a monitored code construct is a set of memory cells 
which contribute RMS+ or represent distinct elements in an array or a linked list. 
In one program run, we assume that we collect a sequence of $m$ samples. 
Given the $i$th sample, we use $M_i$ to represent the 
total number of distinct memory cells in the previous $i-1$ samples, 
$C_i$ to represent the number of distinct memory cells in the $i$th sample,
and $R_i$ to represent the number of distinct memory cells in 
the $i$th sample and also appeared in one of the previous $i-1$ samples.
The total distinct memory cells for all dynamic instances 
of the monitored code construct can be estimated as:


\begin{equation} \label{eq:mark}
%$$p_v(S_v(a)) = 1 - \prod\limits_{u \in S_v(a)}(1 - p_{u,v})$$
N = \frac{\sum\limits_{i=1}^m M_i*C_i}{\sum\limits_{i=1}^m R_i}
\end{equation}

\noindent\textbf{Cost Metric}
The production-run version of \Tool focuses on recursive functions or loops.
If the monitored code construct is a recursive function,
we will use the total number of recursive call instances to estimate cost.
If the monitored code construct is a loop, 
we will use the total number of loop iterations for all loop instances to estimate cost. 
We do not apply sampling during collecting these two metrics. 
As we discussed in Section~\ref{sec:inhouse},
these two metrics will incur a smaller overhead compared with BBs, 
and they can still provide accurate profiling results.  


\subsection{Experimental Evaluation}
%\subsubsection{Research Questions}

We will conduct experiments to answer the following two research questions:

\begin{itemize}
\item {\bf RQ1.} 
Can sampling lower the runtime overhead while maintaining the same profiling results? 
A positive answer means the effectiveness of the production-run version of \Tool. 

\item {\bf RQ2.} 
Will sampling increase profiling latency? 
After applying sampling, less information is collected in one single run. 
If we have to monitor more program runs in order to get the same profiling results,
profiling latency is increased.
As we discussed earlier, the majority of complexity problems are caused 
by repeated executions of a code construct. 
It could be possible that sampling can still get enough information in one program, 
while not increasing diagnosis latency. 

\end{itemize}

\noindent\textbf{Benchmarks and Inputs}
We reuse all benchmarks in Table~\ref{tab:benchmark_info}.
We monitor the loop or the recursive function with largest iteration 
numbers\footnote{We consider a recursive function call instance as a loop iteration.} 
for all benchmarks,
except for Mozilla\#347306 and GCC\#46401. 
These two bugs are caused repeated execution of a loop, 
whose total iteration numbers ranked 2nd. 
We monitor these two loops.

For each benchmark, 
we use the same methodology in Section~\ref{sec:inhouse_exp} 
to generate a sequence of inputs, 
so that the difference between sizes of two consecutive inputs is constant.
We name this input set as controlled inputs.
We also randomly sample controlled inputs to generate a set of random 
inputs to emulate inputs in production runs.    

\noindent\textbf{Metrics}
We will measure the following three metrics during our experiments:
1) runtime overhead, which is the slow down caused 
by sampling information in each program run;
2) profiling capability, which is measured as the similarity between two cost functions 
inferred under in-house setting and production-run setting. 
3) profiling latency, which is measured by how many program 
runs are needed to conduct the algorithmic profiling. 

\input{section/tab_online1}


\noindent\textbf{Settings}
By default, we set the sampling rate to be 1 out of 100 
and conduct algorithmic profiling by using 100 program runs with controlled inputs.  

Besides the default setting,
we evaluate the impact of different number of monitored program runs, 
ranging from 10 to 1000, under the default sampling rate 1 out of 100.
We conduct this experiment by using both controlled inputs and random inputs 
to understand whether different input sets will influence experimental results.
We also evaluate the impact of sampling rate, ranging from 1 out of 10 to 1 out of 10000, 
by using 100 program runs with controlled inputs.  

Sampling will introduce some randomness. 
We conduct each experiment multiple times, 
and our results are stable across multiple experiments. 
Particularly, overhead is measured by using 10 runs under the same setting. 



\input{section/tab_online2}
%{\underline{\textit{Input Size Metric.}}}
















%\input{section/tab_online1}
%\input{section/tab_online2}




%\subsection{Discussions}

