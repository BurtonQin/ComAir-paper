\section{Understanding Real-World Complexity Bugs}
\label{sec:study}


In this section, we conduct an empirical study on real-world complexity bugs. 
Specifically, we try to answer the following two questions:

\begin{enumerate}

\item What are common root causes for complexity bugs? 
Here, we consider code constructs conducting computation in unexpected complexity as root causes. 
Studying root causes can help us understand whether or not 
existing algorithmic profiling algorithms work on real-world bugs 
and can also guide the design of new approximate algorithms. 

\item How real-world complexity bugs are reported and diagnosed?
This can help us understand the state of practice of combating process for complexity bugs. 

\end{enumerate}


\subsection{Methodology}
\label{sec:meth}

We conduct our empirical study based on a public benchmark set for 
real-world performance bugs~\cite{PerfBug,SongOOPSLA2014}. 
In the previous work~\cite{PerfBug}, 
the authors collected 110 real-world performance bugs from 5 representative 
software suites, which are Apache, Chrome, GCC, Mozilla, and MySQL. 
These software suites cover various types of functionalities and are implemented 
by different programming languages, including C/C++, Java, C\#, and JavaScript. 
These software suites are all large and mature, 
with millions of lines of codes and long development histories. 
In their following work~\cite{SongOOPSLA2014}, 
the authors further identified 65 user-perceived performance bugs, 
which are all observed and reported by users. 


Our study focuses on user-perceived performance bugs, 
since these bugs have large performance impact.
After carefully reading bug reports and buggy code fragments of these bugs,
we clearly identify \ComBugs performance problems 
caused by {\textit{unexpected algorithmic complexity}}.
These performance problems will be referred to as 
{\textit{complexity problems}} in the reminder of this paper.


{\bf{\textit{Caveats.}}}
The same as all previous empirical studies, 
our findings and conclusions need to be considered together with our methodology.
All studied complexity bugs come from representative software. 
However, there are other types of software not covered in our study, 
such as distributed systems and HPC software. 
All studied complexity problems are reported by users through bug tracking systems.  
We do believe that there are complexity problems noticed 
and fixed by developers in other ways. 
However, there are no conceivable methods to study them.
We believe that the studied complexity problems can serve as a representative sample
of complexity problems in the real world. 


\subsection{Root causes of complexity problems}

\input{section/tab_study}

In this section, we study root causes of complexity problems in different categories. 
Our study focuses on 
1) what code constructs causing the complexity problems, 
2) why the complexity problems can generate user-perceived performance impact, 
and 3) how developers fix these complexity problems. 


{\underline{\textit{$O(N)$: linear complexity.}}} 
As shown in Table~\ref{tab:study}, 
8 out of \ComBugs studied complexity problems are in linear complexity. 
All of these problems are caused by a buggy loop, 
whose average loop iteration number scales in term of input size $N$.

For 5 of them, their corresponding buggy loops contain serialized I/O operations.
Although average loop iteration numbers or input sizes are not large,
these bugs are still perceived by users.
The patches for these bugs are to aggregate I/O operations 
or completely eliminate unnecessary I/O operations.   
For example, Mozilla\#490742 is caused by bookmarking tabs 
by using separated database transactions. 
Even bookmarking 50 tabs can cause a timeout dialog to pop up during a buggy run.
To fix this bug, mozilla developers use only one aggregated transaction to bookmark all tabs together.
As another example, Mozilla\#344059 is due to saving unchanged 
search engine preferences to SQLite, 
and it is fixed by only saving search engine preferences when some of them are changed.


For the other 3 bugs, their buggy loops will execute many iterations during buggy runs,
and they are fixed by adding shortcuts to completely remove the buggy loops.
Take MySQL\#33948 as an example, 
originally MySQL developers keep both used and free table entries in the same linked list,
and the buggy loop is to go through the list and look for a free entry.
During buggy runs, many table entries are used and the buggy loop 
needs to take a lot of iteration to find a free entry. 
To fix this bug, MySQL developers simply keep used entries 
and free entries in two different linked lists. 


{\underline{\textit{$O(N^k)$: polynomial complexity (k>1).}}}
As shown in Table~\ref{tab:study}, 
more than half of the studied complexity bugs are in polynomial complexity. 
Similar to complexity problems in linear complexity,
polynomial complexity for each bug in this category is caused by a buggy loop.
However, {\bf both} loop execution numbers and average loop iteration number
scale as input size $O(N)$.



\begin{figure}
\centering
\lstset{basicstyle=\ttfamily\fontsize{7}{8}\selectfont,
     morekeywords={+},keepspaces=true}
  \mbox{\lstinputlisting[mathescape,boxpos=t]{figure/mysql27287.c}}
\caption{A MySQL performance problem in polynomial complexity. 
  (During buggy runs, the total loop iterations scale polynomially in the size of \texttt{items}.)}
\vspace{-0.05in}
\label{fig:mysql27287}
\vspace{-0.05in}
\end{figure}

To fix the majority (13/18) of performance problems in this category, 
developers directly modify the loops whose total loop iterations scale polynomially.  
The buggy loop for MySQL\#27287 is shown in Figure~\ref{fig:mysql27287}.
The loop searches parent \texttt{XML\_NODE} for the function input \texttt{nitems}, 
which is the index for another \texttt{XML\_NODE}.
All \texttt{XML\_NODE}s are maintained in the array \texttt{items}. 
The way the loop to conduct the search is to iterate array \texttt{items} 
backward from the input node and look for the first \texttt{XML\_NODE} 
whose level is one less than the input.
During buggy runs, one \texttt{XML\_NODE} contains tens of thousands of children, 
and \texttt{xml\_parent\_tag} is invoked for each of its children. 
To fix this bug, MySQL developers add an extra field to each 
\texttt{XML\_NODE} to save its parent, 
and this field is initialized when a \texttt{XML\_NODE} is created. 
After that, the buggy loop is completely removed. 

To fix other performance problems (5/18) in this category, 
developers reduce data processed by the loops scaling polynomially, 
instead of optimizing the loops directly. 
For example, the loop scaling polynomially for GCC\#12322 is part 
of the functionality to do basic block reordering, 
and it will perform poorly for basic blocks with many successors. 
The bug-triggering input contains a basic block with thousands of successors. 
To fix this, developers simply skip basic blocks with many successors, 
which will lose some optimization opportunities 
but can still keep generated codes working correctly. 


\begin{figure}
\centering
\lstset{basicstyle=\ttfamily\fontsize{7}{8}\selectfont,
     morekeywords={+},keepspaces=true}
  \mbox{\lstinputlisting[mathescape,boxpos=t]{figure/gcc27733.c}}
\caption{A GCC performance problem in exponential complexity. 
 (During buggy runs, how many times \texttt{mult\_alg} is invoked scales exponentially
  in the number of ones in the binary form of input \texttt{t}.)}
\vspace{-0.05in}
\label{fig:gcc27733}
\vspace{-0.05in}
\end{figure}


{\underline{\textit{$O(e^N)$: exponential complexity.}}}
4 studied complexity problems are in exponential complexity. 
These complexity problems are fixed by 
either leveraging memoization to reuse previous results 
or skipping computation with exponential complexity for large workload. 

3 of the performance problems are caused by recursive function calls. 
Take GCC\#27733 in Figure~\ref{fig:gcc27733} as an example, 
recursive function \texttt{mult\_alg} computes the best algorithm to multiply \texttt{t}.
In each invocation, \texttt{mult\_alg} will try a set of bitwise 
operations to change input 
\texttt{t} into a smaller number \texttt{t'} and recursively call itself. 
The number of times \texttt{mult\_alg} scales exponentially 
in terms of 1s in the binary form of input \texttt{t}. 
To optimize this function, 
GCC developers use a hash table \texttt{alg\_hash} to record
which \texttt{t} has been processed before and what is its corresponding result.
However, there is a type declaration error inside the hash table entry struct,
and this error causes \texttt{t} larger than the maximum unsigned integer to never hit cache.
For large \texttt{t}, \texttt{mult\_alg} is still in exponential complexity. 
To fix this bug, 
GCC developers simply change the type declaration to enable memoization for large \texttt{t}. 

GCC\#32540 is in exponential complexity and is caused by an inefficient loop. 
The loop applies iterative algorithm to implement a compiler optimization. 
The bug-triggering input contains very complex control and data dependence,  
so that the buggy loop scales exponentially in terms of \texttt{if} branch in the input. 
To fix this bug, developers simply disable optimization 
after detecting complex control and data dependence.  


\subsection{Diagnosis process of complexity problems}

We also studied how complexity problems are reported by users and get diagnosed by developers. 

{\underline{\textit{How complexity problems are reported?}}
As shown in Table~\ref{tab:study},
for most of complexity bugs (25/35), 
how to change input sizes in order to observe performance scaling problems is specified by users during reporting. 
As discussed in previous works~\cite{SongOOPSLA2014}, 
all collected user-perceived performance bug reports in the benchmark set 
contain bug-triggering inputs from users. 
For most complexity bugs, 
users will also describe how they change input sizes 
and how performance changes accordingly. 
For example, when reporting GCC\#32540, 
the user provides a C file and several measurement results to 
show how GCC compilation time scales exponentially with the number of branch in the C file. 
As another example, when reporting Mozilla\#490742, 
the user mentions that the length of high, 
flat CPU usage is related to the number of search engine preferences. 

For the other 5 bugs, users provide one bug-triggering inputs or describe a set of bug-triggering inputs during reporting. 
For example, when reporting GCC\#27733, 
the user provides one bad input, which can take GCC 49 minutes to compile. 
When reporting Mozilla\#231300, the user mentions that clearing disk cache on Mac can trigger the bug, 
no matter what is the content of the cached files. 

{\underline{\textit{How complexity problems are diagnosed?}}
We study how long it takes developers to fix complexity problems. 
On average, it takes developers 162 days to diagnose a complexity problem. 
Following the same methodology as the previous work~\cite{SongOOPSLA2014},
we calculate diagnosis time starting from when a problem was reported 
and ending at when the problem was fixed by developers. 
It takes developers 101 days to diagnose non-complexity bugs in the same set on average. 
Diagnosing complexity bugs takes longer time. 

As discussed in the previous works~\cite{SongOOPSLA2014}, 
profiler is the only diagnosis tool mentioned in bug reports 
when diagnosing the user-perceived performance problems. 
Profiler is mentioned in 7 out of 30 complexity bug reports. 
The percentage of bug reports where profiler is mentioned is similar compared complexity bugs with non-complexity bugs. 


\subsection{Implications}
{\underline{\textit{Implication 1}}
All studied complexity problems are caused either by loops or by recursive function calls. 


{\underline{\textit{Implication 2}}
How to change input size is described by users, 
and this is very important to profile programs’ complexity and diagnose complexity problems.  

{\underline{\textit{Implication 3}} 
Profiling may not be that useful. 
