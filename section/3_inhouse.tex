
\newpage
\section{In-house Algorithmic Profiling}
\label{sec:inhouse}

In this section, we discuss the design and implementation for the in-house version of \Tool.
Under the in-house setting, 
\Tool can help developers conduct algorithmic profiling to detect 
previously unknown complexity problems before releasing their software, 
or help developers diagnose and fix complexity problems reported by end users. 

To conduct algorithmic profiling,
we first need to record \textit{input} size and \textit{cost} for different code constructs 
during multiple executions;
we then plot records from the same code construct with input size as x-axis and cost as y-axis; 
and finally, we infer a cost function of the input size.
The code constructs could be loops and functions. 
In order to design \Tool, 
there are three fundamental questions we need to answer:
1) how to design input metric; 2) how to design cost metric; 
and 3) how to infer and compare cost functions.


\subsection{Input Metric Design}
\label{sec:input}

The goal of input metric design is to figure out general metrics, 
which can represent input sizes for different code constructs. 
We cannot rely on developers to manually label or specify input information 
for different code constructs, 
because it is time-consuming and also difficult for complex code constructs.  
There are several metrics, which can be used to measure input for a code construct.
We discuss commonly used ones as follows. 

\subsubsection{Program input}
As discussed in Section~\ref{sec:process}, 
users tend to specify how to change the whole program 
input to describe the perceived complexity problem.
It is fairly easy to measure the input size for the whole program based on users' reports.
One way to measure the input size for a code construct inside a program
is to simply use the input size of the whole program. 
However, the whole program input 
is related to the input of a code construct in various ways.
Changing the whole program input may not change input sizes for 
all code constructs. 
Using the input size of the whole program as an input metric for a code 
construct may lead to incorrect profiling results. 

\subsubsection{Read memory size (RMS)}
RMS is proposed as an input size metric for one 
execution of a code construct~\cite{Aprof1,Aprof2}. 
RMS is defined as the number of distinct memory cells 
whose first access is read. 
RMS considers both read accesses conducted by a code construct directly 
and read accesses conducted by 
functions called from the code construct. 
RMS is a generic metric for input size, 
and it can provide important input information for many complexity problems.   
Given a code construct \texttt{c}, which is inside a loop \texttt{l} and executed
$n$ times in one single run, 
we would have $n$ RMS records for \texttt{c}. 
There are two methods to analyze these $n$ RMS records.

\begin{itemize}

\item First, \textit{top-down} method. 
When analyzing complexity for \texttt{c}, 
we consider the $n$ records independently from each other~\cite{Aprof1,Aprof2}. 
The effect of multiple executions of \texttt{c} 
will be aggregated when analyzing complexity for \texttt{l}.

\item Second, \textit{bottom-up} method. 
When analyzing complexity for \texttt{c}, 
we merge the $n$ records and calculate the number of distinct memory cells 
contributing RMS as the input size of \texttt{c} in the program run.

\end{itemize}



\begin{figure*}
\centering
\subfloat[XXXX]{\includegraphics[width=0.22\linewidth]{figure/mysql27287-complexity-n-square}\label{fig:mysql27287-indep}} 
\subfloat[XXXX]{\includegraphics[width=0.22\linewidth]{figure/mysql27287-complexity-n-square}\label{fig:mysql27287-outer}}
\subfloat[XXXX]{\includegraphics[width=0.22\linewidth]{figure/gcc27733-time-cost-line}\label{fig:mysql27287-merge}} 
\subfloat[XXXX]{\includegraphics[width=0.22\linewidth]{figure/gcc27733-time-cost-line}\label{fig:mysql27287-inner-array}} \\ 
\vspace{-0.1in}
\caption{XXXX
\footnotesize{(These figures show how execution time change with the change of input size for MySQL\#27287, 
 Apache\#34464, Mozilla\#477564, and GCC\#27733. For each complexity problem, we use 10 distinct inputs.)}} 
\label{fig:time} 
\end{figure*} 

Take MySQL\#27287 as an example.
RMS for one single execution of
the buggy loop in Figure~\ref{fig:mysql27287}
is roughly equal to 2 times the number of \texttt{XML\_NODE} 
accessed during the execution, 
because the \texttt{level} field and the \texttt{type} field of 
different \texttt{XML\_NODE}s are read in different loop iterations.
Although variable \texttt{p} and \texttt{level} are also read in each iteration,
RMS only considers distinct memory cells and 
only increments its value for the first read on these two variables in the first iteration. 
The outer loop, not shown in Figure~\ref{fig:mysql27287}, 
invokes function \texttt{xml\_parent\_tag} for every \texttt{XML\_NODE} contained
in the same array \texttt{items}.
If we use the top-down method to analyze the buggy loop
(or function \texttt{xml\_parent\_tag}), 
execution cost is in linear relationship with RMS, 
as shown in Figure~\ref{fig:mysql27287-indep}.
The $O(N^2)$ complexity can be inferred from the side of the outer loop, 
as shown in Figure~\ref{fig:mysql27287-outer}. 
If we use the bottom-up method, 
we can observe aggregated cost scales 
polynomially in terms of RMS for the buggy loop 
(or function \texttt{xml\_parent\_tag}), 
which is shown in Figure~\ref{fig:mysql27287-merge}. 


Both of the two methods can infer the same complexity, such as for MySQL\#27287,
but they require different implementations.
If we want to apply the bottom-up method to analyze a code construct,
we need to track distinct memory cells contributing RMS for the code construct.
During in-house testing, developers usually want to conduct algorithmic profiling 
for all executed code constructs. 
Tracking memory cells for every code construct will 
incur a very large memory overhead.  
As we will discuss later, to infer complexity in the top-down way 
is not suitable in production runs. 
Therefore, for in-house setting, we will the top-down method, 
while we will use the bottom-up under production-run setting. 

Since RMS considers memory accesses conducted by callees 
and we take a top-down method for in-house setting,
when a monitored program conducts a memory access,
we need to check functions active on the call stack and 
update information necessary to calculate RMS. 
For simplicity, we use function as granularity for code constructs under in-house setting.
Otherwise, we need to check whether a monitored code construct enclose related call sites, 
when we check functions active on the call stack.  





\subsection{Cost Design}

\subsubsection{Executed basic blocks (BBs)}
The number of executed BBs for a dynamic instance of a code construct
can be used to measure the execution cost for the dynamic instance. 
A naive method to collect this metric is to add a global counter 
to the monitored program and increase the counter value by 1 inside each BB.  
When entering and leaving a monitored code construct, 
the value of the counter will be dumped to log.

To efficiently count executed BBs, 
we apply an algorithm, which was originally designed to 
efficiently count edge events through selectively instrumenting a counter 
on CFG~\cite{event-counting}.
The algorithm has already been proved to be able to 
conduct path profiling efficiently~\cite{peter-ase,path-profiling}. 

To apply the algorithm,
we instrument a local counter \texttt{local\_cost} for each function
and initialize its value to be 0 at the entry BB. 
We will add the value of \texttt{local\_cost} to a global counter \texttt{cost} 
at the exit BB.
After that, we only need to consider where 
and how to update \texttt{local\_cost} 
within a single function.
We design and implement an intro-procedural control flow analysis
to achieve this.
Given a single function,
we add a fake edge from the exit BB to the entry BB 
to make its CFG strongly connected. 
Since the original algorithm is design to count edge events,
we split each BB into two 
and label the event number to be 1 for each edge connecting a pair of split BBs 
We label the event number to be 0 for all other edges.
We compute a spanning tree~\cite{spanning} for the new CFG.
Edges not in the spanning tree are called chords.
We apply the depth-first search algorithm proposed in~\cite{event-counting} 
to calculate on which chords we should change 
\texttt{local\_cost} 
and how much we should change.

If a monitored code construct is not a function,
We need to figure out the accurate value for the global counter \texttt{cost} 
before dumping its value 
at the first and last BB of the monitored code construct.
The algorithm we apply is also discussed in~\cite{event-counting}.




\input{section/tab_inhouse1}
\input{section/tab_inhouse2}