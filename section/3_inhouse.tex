
%\newpage
\section{In-house Algorithmic Profiling}
\label{sec:inhouse}

In this section, we discuss the design and implementation for the in-house version of \Tool.
Under the in-house setting, 
\Tool can help developers conduct algorithmic profiling to detect 
previously unknown complexity problems before releasing their software, 
or help developers diagnose and fix complexity problems reported by end users. 

To conduct algorithmic profiling,
we first need to record \textit{input} size and \textit{cost} for different code constructs 
in multiple program runs;
we then plot records from the same code construct with input size as x-axis and cost as y-axis; 
and finally, we infer a cost function of input size.
To design the in-house version of \Tool, 
we need to answer three fundamental questions:
1) how to design input metrics; 
2) how to design cost metrics;
and 3) how to infer and compare cost functions. 

\subsection{Input Metric Design}
\label{sec:input}

The goal of input metric design is to figure out metrics
holding input size information for different types of code constructs. 
We cannot rely on developers to manually label or specify input size information, 
because it is time-consuming and also difficult for complex code constructs.  
There are several metrics, which can be used to measure input size.
We discuss them as follows. 

\noindent\textbf{Program Input}
As discussed in Section~\ref{sec:process}, 
users tend to specify how to change the whole program 
input to describe the perceived complexity problem.
It is fairly easy to measure input size for the whole program based on users' reports.
One way to measure input size for a code construct
is to simply use the input size of the whole program. 
However, the whole program input 
is related to the input of a code construct in various ways.
Changing the whole program input may not change input sizes for 
all code constructs. 
Using the input size of the whole program as input size for a code 
construct may lead to incorrect profiling results. 

\noindent\textbf{Read Memory Size (RMS)}}
RMS is proposed as an input metric for a dynamic instance
of a code construct~\cite{Aprof1,Aprof2}. 
RMS is defined as the number of distinct memory cells 
whose first access is read. 
RMS considers both reads conducted by a code construct directly 
and reads conducted by 
functions called from the code construct. 
RMS can provide important input information for many complexity problems.   
 
Given a code construct \texttt{c} of a program, 
we assume \texttt{c} is inside a loop \texttt{l} and is executed
$n$ times in one run of the program. 
We would have $n$ RMS records collected 
for the $n$ dynamic instances of \texttt{c}. 
There are two methods to analyze these $n$ RMS records.
The first method aggregates the $n$ records in a \textit{top-down} way~\cite{Aprof1,Aprof2}.
When analyzing complexity for \texttt{c}, 
we consider the $n$ records independently from each other. 
The effect of multiple instances of \texttt{c} 
will be aggregated when analyzing complexity for \texttt{l}.
The second method aggregates the $n$ records in a \textit{bottom-up} way.
When analyzing complexity for \texttt{c}, 
we merge the $n$ records and calculate the number of distinct memory cells 
contributing RMS as the input size 
of \texttt{c} during the whole program run.



\begin{figure*}
\centering
\subfloat[Inner]{\includegraphics[width=0.22\linewidth]{figure/mysql27287-inner-loop-line}\label{fig:mysql27287-indep}} 
\subfloat[Outer]{\includegraphics[width=0.22\linewidth]{figure/mysql27287-outer-loop-line}\label{fig:mysql27287-outer}}
\subfloat[Bottom-up]{\includegraphics[width=0.22\linewidth]{figure/mysql27287-loop-bb-line}\label{fig:mysql27287-merge}} 
\subfloat[Array]{\includegraphics[width=0.22\linewidth]{figure/mysql27287-loop-array-line}\label{fig:mysql27287-inner-array}} \\ 
\vspace{-0.1in}
\caption{How cost scales for MySQL\#27287. 
\footnotesize{(These figures show how cost scales using RMS or \# of array elements 
as input metric for different code constructs of MySQL\#27287. 
The first two figures are analyzed using the top-down method, 
and the last two are analyzed using the bottom-up method.)}} 
\label{fig:cost} 
\vspace{-0.15in}
\end{figure*} 
 

Take MySQL\#27287 as an example.
RMS for a dynamic instance of
the buggy loop in Figure~\ref{fig:mysql27287}
is roughly equal to 2 times the number of \texttt{XML\_NODE}s 
accessed during the execution, 
because the \texttt{level} field and the \texttt{type} field of 
different \texttt{XML\_NODE}s are read in different loop iterations.
Although variable \texttt{p} and \texttt{level} are also read in each iteration,
RMS only considers distinct memory cells and 
only increments its value for the first read on these two variables in the first iteration. 
The outer loop, not shown in Figure~\ref{fig:mysql27287}, 
invokes \texttt{xml\_parent\_tag()} for every \texttt{XML\_NODE} contained
in the same array \texttt{items}.
If we use the top-down method to analyze the buggy loop
or the buggy function, 
execution cost is in linear relationship with RMS, 
as shown in Figure~\ref{fig:mysql27287-indep}.
The $O(N^2)$ complexity can be inferred from the side of the outer loop, 
as shown in Figure~\ref{fig:mysql27287-outer}. 
If we use the bottom-up method, 
we can observe the aggregated cost scales 
polynomially in terms of RMS of the buggy loop 
or the buggy function, 
which is shown in Figure~\ref{fig:mysql27287-merge}. 

\noindent\textbf{Data structure size (DSS)}
The number of distinct accessed elements of a data 
structure could also be used to measure 
input size for a dynamic instance of a code construct~\cite{AlgoProf}. 
%As we discussed in Section~\ref{sec:study_impli},
%array and linked list are the two types of data structures
%most commonly involved in complexity problems. 

Similar to RMS, there are also, top-down and bottom-up, 
two methods to analyze 
multiple DSS records collected for the same code 
construct in one program run.
Take MySQL\#27287 in Figure~\ref{fig:mysql27287} as an example.
If we focus on array \texttt{items},
DSS for one execution of the inner loop is the number of 
accessed array elements during the execution.
If we use the top-down method, the execution cost 
of the inner loop is in linear relationship with its DSS.
For the outer loop, its execution cost is in $O(N^2)$ relationship with its DSS,
since DSS only considers distinct accessed elements.
If we apply the bottom-up method, 
we count distinct accessed array elements across all dynamic instances of the inner loop,
the aggregated execution cost of the inner loop is in $O(N^2)$ relationship with its DSS, 
as shown in Figure~\ref{fig:mysql27287-inner-array}.   

\noindent\textbf{Library input size (LIS)}
There are code constructs taking inputs from library calls. 
As we discussed in Section~\ref{sec:tax}, 
the loop shown in Figure~\ref{fig:apache34464} 
is in $O(N^2)$ complexity in terms of the number 
of characters returned from \texttt{getchar()}. 
Besides \texttt{getchar()}, there are many other library calls, 
which allow code constructs to take inputs
from disk, terminal, network, and many other places. 

LIS can complement RMS for cases like Apache\#34464 in Figure~\ref{fig:apache34464}.
RMS only considers memory cells whose first access is read. 
The first accesses for all memory cells of string \texttt{source} are writes on line $8$.
Therefore, all accesses on string 
\texttt{source} will not increase RMS value.  
%If we use RMS to measure input size for the loop in Figure~\ref{fig:apache34464},
RMS for the loop in Figure~\ref{fig:apache34464} only depends 
on the size of string \texttt{target}, 
variable \texttt{sourceLen}, and variable \texttt{targetLen}, 
regardless of the number of characters from \texttt{getchar()}.
DSS can correctly measure input size for Apache\#34464,
since all characters from \texttt{getchar()} will be saved into 
string (or array) \texttt{source},
and each element of string \texttt{source} 
will be accessed by the inner loop inside \texttt{indexOf()}. 


\begin{figure}
\centering
\lstset{basicstyle=\ttfamily\fontsize{7}{8}\selectfont,
     morekeywords={+},keepspaces=true,numbers=left}
  \mbox{\lstinputlisting[mathescape,boxpos=t]{figure/fib.c}}
  \vspace{-0.1in}
\caption{A recursive function exponential complexity.
\footnotesize{(This figure shows a recursive implement to compute fibonacci numbers. 
Execution time scales polynomially in the value of input parameter $n$.)}}
\vspace{-0.1in}
\label{fig:fib}
\vspace{-0.15in}
\end{figure}


\noindent\textbf{Important integer variable (IIV)}
Sometimes, a code construct contains some important integer variables,
whose values determine how many times a loop iterates 
or how many times a recursive function is invoked. 
Values of these important integer variables contain important information about input sizes. 
For example, a recursive implementation to calculate fibonacci numbers is shown in Figure~\ref{fig:fib}, 
and it scales exponentially in the value of variable $n$.

IIV can complement RMS, DSS and LIS for cases like 
computing fibonacci numbers in Figure~\ref{fig:fib}. 
If we use RMS to measure input size,
no matter how large \texttt{n} is, the input size will always be constant. 
For an invocation of \texttt{fib},
the first read on parameter \texttt{n} will increase RMS.
After that, RMS will not increase any more.
This is because whenever \texttt{fib} recursively call itself on line 3,
it must write parameter $n-1$ (or $n-2$) onto the stack, 
which means the first access for the stack cell is write 
and future accesses on the stack cell will not increase RMS. 
The recursive function does not
involve any data structure or library call, 
so that it cannot be measured by DSS and LIS. 
 

\noindent{\textit{\underline{Input metric selection for \Tool.}}}
We choose input metrics to explore two directions. 
The first one is to cover as many cases as possible. 
RMS is the most general metric.
RMS can largely cover cases DSS can handle.
We use LIS and IIV to enhance RMS.
We name the enhanced version as RMS+.
The second direction is to find a simple and semantic metric, 
which can cover  a good percentage of common cases. 
We will apply DSS along this direction. 
In the remainder of this paper, 
we will focus on RMS+ and DSS as input metrics. 

\subsection{Cost Metric Design}
When a code construct is executed, 
it could consume many different types of resources, 
such as computation, network bandwidth, storage, and so on.
In this paper, we focus on how to measure computation cost and 
leave the measurement for other types of consumed resources for future works. 
When a code construct is executed, 
there are many metrics can be used to measure computation 
cost for a dynamic instance of the code construct. 
We discuss commonly used ones as follows.

\noindent\textbf{Execution Time}
We can use time elapsed during executing a dynamic instance to represent computation cost 
for the instance.
One potential problem to use execution time is that, 
when a dynamic instance takes very little time, 
execution time may fail to provide an accurate measurement.  

\noindent\textbf{Executed Basic Blocks (BBs)}
We can count the number of executed BBs for a dynamic instance
and use this number as computation cost. 
The assumption to use this metric is that 
each executed BB will roughly consume the same computation cost. 

\noindent\textbf{Executed Instructions}
We can use the executed instruction number of a dynamic 
instance to represent computation cost for the instance. 
Similar to executed BBs, we assume each instruction 
roughly takes the same computation cost by using this metric. 


\noindent\textbf{{Loop Iterations (LIs)}}
As we discussed in Section~\ref{sec:study},
many complexity problems are caused by a loop.
If we want to measure computation cost for a loop instance,
we can use the number of loop iterations.
By using this metric, we assume that computation 
cost spent in each iteration is roughly the same. 

\noindent\textbf{Recursive Invocations (RIs)}
As we discussed in Section~\ref{sec:study},
there are also complexity problems caused by recursive functions.
If we want to measure computation cost for a dynamic instance \texttt{d}
of a recursive function \texttt{f}, 
we can use the number of times when
\texttt{d} recursively call \texttt{f} directly or indirectly. 
By using this metric, 
we assume that except the recursive part, 
computation spent in other parts is roughly 
the same across different recursive instances. 

\noindent{\textit{\underline{Cost metric selection for \Tool.}}}
Similar input metrics,
we want both generic cost metrics and cost metrics that can cover common cases. 
We use BBs as the generic metric.
We do not use execution time, 
because it is not accurate enough for dynamic instances taking too little time.
We do not use the number of executed instructions, 
because this number is highly correlated with BBs.  
All our studied complexity problems are caused either by a loop or a recursive function. 
Therefore, we will use LIs and RIs as cost metrics for loops and recursive functions, 
respectively, in order to cover common cases. 


\subsection{Cost Function Inference and Comparison}

\noindent\textbf{Inference}
After collecting a set of (input size, cost) pairs for a code construct,
we first use the maximum cost value to aggregate pairs with the same input size into one.
We do this, because we want to infer the worst-case complexity for the code construct. 
To infer a cost function,
we consider four possibilities: 
an exponential function, or a polynomial function with degree from 1 to 3.
We use least square~\cite{least} to determine parameters for the four possible functions. 
For example, we start a polynomial function with degree 2 as $y=a*x^2 + b*x +c$.
We choose $a$, $b$ and $c$ to minimize the sum of squared difference between 
observed value and fitted value.  
To choose the best one among the four possible fitted functions, 
we calculate coefficient of determination for each. 
Coefficient of determination is commonly used to measure goodness-of-fit~\cite{codeter}.
We choose the fitted function with highest coefficient of determination. 


\noindent\textbf{Comparison}
We have more than one metric for both input and cost.
We need to compare functions inferred under different metrics. 
After collecting two sets of records, $a$ and $b$, using different metrics, 
we do the comparison through the following steps. 
We first standardize the two sets of data. 
We then infer two cost functions, $f_a()$ and $f_b()$, 
by using the two sets of standardized data.
We randomly sample a set of $n$ numbers from $(-1, 1)$.
We use $N$ to represent the set of the $n$ sampled numbers.  
We calculate the similarity between the two inferred 
functions by using the following formula. 


%\begin{equation} \label{eq:fun_similarity}
%$1-|f_a(x) - f_b(x)|/(max(f_a(x),f_b(x))+1)$
%\end{equation}

\begin{equation} \label{eq:sim}
%$$p_v(S_v(a)) = 1 - \prod\limits_{u \in S_v(a)}(1 - p_{u,v})$$
S(f_a, f_b) = 1 - \frac{1}{n}\sum_{x \in N}\frac{|f_a(x) - f_b(x)|}{max(f_a(x),f_b(x))+1}
\end{equation}

The intuition of Equation~\ref{eq:sim} is to calculate the area under the two curves 
plotted by $f_a()$ and $f_b()$, 
and compute the similarity as the percentage of the area under both of the two curves, 
compared with the area under either of the two curves.
Since the minimum value of standardized data is $-1$ not $0$,
we minus $-1$ in the denominator.
The maximum value of $S(f_a, f_b)$ is $1$, and the minimum value is $0$.
A larger result means the two functions inferred 
from $a$ and $b$ are more similar to each other. 



\subsection{Implementation and Optimization}

We use LLVM~\cite{llvm} to instrument programs to be profiled.
We use python to analyze profiling records 
with input and cost information and to infer the cost function. 
We also design several optimizations to lower the runtime overhead 
during dynamic monitoring.  

\subsubsection{Input Metrics}
As we discussed in Section~\ref{sec:input},
both RMS+ and DSS have, top-down and bottom-up, 
two methods to analyze profiling records for multiple instances of 
a code construct in one program run. 
Both of these two methods can infer the same complexity, 
such as for MySQL\#27287,
but they require different implementations.
If we want to apply the bottom-up method,
we need to track distinct memory cells already contributing 
RMS+ or DSS for every code construct under monitoring.
During in-house testing, developers usually want to conduct algorithmic profiling 
for all executed code constructs. 
Tracking memory cells for every code construct will 
incur a very large memory overhead.  
As we will discuss later, the top-down method 
is not suitable in production-run usage. 
Therefore, we apply the top-down method under in-house setting, 
and we will use the bottom-up method under production-run setting. 

Since we take the top-down method for in-house setting,
when a monitored program conducts a memory access,
we need to check all functions active on the call stack and 
update information necessary to calculate RMS+ or DSS. 
For simplicity, we use function as granularity for code constructs under in-house setting.
Otherwise, we need to check whether a monitored code construct enclose related call sites, 
when checking active functions on the call stack.  

{\textit{\underline{How to implement RMS+?}}}
\citet{Aprof1,Aprof2} designed an algorithm to collect 
RMS for all executed functions based on 
Valgrind. We briefly discuss this algorithm firstly, 
and then we will discuss how to enhance this algorithm. 
To count RMS for every function call instance, 
several global variables are maintained: 
\texttt{count}, serving as the current timestamp and incremented 
by 1 after each function invocation,  
\texttt{ts}, a hash table containing the latest access timestamp for each memory cell, 
and \texttt{S}, a shadow stack tracking all active functions on the stack. 
Four hook functions are instrumented for four types of instructions:
\texttt{call}, \texttt{return}, \texttt{read}, and \texttt{write}.
The \texttt{call} hook function will increment the timestamp variable \texttt{count} 
and grow the shadow stack \texttt{S}.
The \texttt{return} hook function will generate a 
log containing information about the returning function's RMS.
The \texttt{read} hook function will query the hash table \texttt{ts} to decide whether 
to increment RMS for all the functions active on the shadow stack \texttt{S}.
Both the \texttt{read} and \texttt{write} hook 
functions will update the hash table \texttt{ts}
by using the value of \texttt{count} to guarantee that \texttt{ts} 
contains the latest access timestamp for each memory cell. 

To enhance RMS with LIS, we keep a list of library functions, which can take inputs.
When these library functions are called,
we will increase RMS by using the size of inputs from these libraries calls 
for all functions active on the call stack. 
To enhance RMS with IIV, 
we consider cases where a function recursively invoke itself using integer parameters. 
We do not instrument \texttt{write} instructions that store integer 
parameters onto stack when a function recursively calls itself. 
If the first accesses on these stack memory cells are \texttt{read} 
instructions inside recursive callees, 
they can increase RMS. 
By doing this, we basically use the number of integer parameters 
active simultaneously on the recursive call chain, 
not concrete integer values, as input size.

{\textit{\underline{How to implement DSS?}}}
To implement DSS, we focus on array and linked list, 
because they are the most common types of data structures in complexity problems. 
We reuse the framework for RMS+ to collect DSS information.
The difference is that we do not instrument \texttt{write} instructions,
and we only instrument \texttt{read} instructions accessing elements 
in an array or a linked list.
We figure out which pointers pointing to elements in an array 
or a linked list through analyzing loops.

Given a pointer, 
if it is deferenced in every iteration of a loop, 
and its value is increased or decreased by an integer value in every iteration,
we consider the pointer points to array elements, 
and the loop is an array-processing loop. 
For example, as shown in Figure~\ref{fig:mysql27287}, 
\texttt{p} points to array elements. 
It is deferenced in every iteration of the loop in Figure~\ref{fig:mysql27287},
and its value is decreased by one in every iteration. 

For a pointer pointing to elements in a linked list,
its base type needs to be a \texttt{struct}. 
The pointer is also deferenced in every iteration of a loop.
Its value is updated in every iteration, 
and its new value is one field of the \texttt{struct} it is pointing to. 
After identifying a pointer pointing to linked list elements,
we consider the loop deferencing the pointer and updating its value as 
a linked-list-processing loop. 
For example, \texttt{n} is deferenced in every iteration of the loop
in Figure~\ref{fig:Mozilla477564}. 
Its value is also updated in every iteration by 
using the \texttt{prev} field of \texttt{node\_t} it is pointing to. 
We identify \texttt{n} pointing to linked list elements,
and the loop in Figure~\ref{fig:Mozilla477564} as a linked-list-processing loop.

{\textit{\underline{How to do optimization?}}}
We design three optimizations to reduce the runtime overhead when collecting input information
from two aspects:
we try to reduce the number of instrumentation sites, 
and we try to accelerate hook functions. 


\textbf{Optimization 1}
The first optimization is designed for RMS+,
and it targets to reduce the number of instrumentation sites 
for \texttt{read} and \texttt{write} instructions. 
The \texttt{read} hook function needs to query hash table \texttt{ts} 
to get the timestamp for the latest access and update RMS+ value when necessary. 
Both \texttt{read} and \texttt{write} hook functions 
need to use the value of \texttt{count} to update hash table \texttt{ts}.
As we discussed earlier, the value of \texttt{count} will be increased 
after each function invocation. 
Therefore, given two consecutive memory accesses on the same memory cell,
if there is no function calls between them, 
we do not need to instrument the second memory access. 
We rely on dominance analysis to implement this optimization. 
%We focus on stack memory cells that hold 
%scalar variables and only have \texttt{read} and \texttt{write} as uses 
%(i.e., not having ``address of'' as uses).
%We only focus on these memory cells,
%because we want to avoid pointer alias analysis, 
%which may potentially introduce inaccurate results. 
For a \texttt{read} or \texttt{write} instruction accessing a memory cell,
if it is dominated by another \texttt{read} or \texttt{write} instruction accessing the same cell, 
and there is not function call on any path between these two instructions,
we will skip the instrumentation for the second instruction.  


\textbf{Optimization 2}}
The second optimization is an attempt to improve the performance of 
lookups in the hash table \texttt{ts}, 
containing the latest access timestamp for each memory cell.
It can work for both RMS+ and DSS.
For a memory read, 
we need to query \texttt{ts} and decide 
whether RMS+ or DSS should be incremented.
For both memory read and write, 
we need to update \texttt{ts} by using the current timestamp \texttt{count}
for accessed memory cells. 
Instead of using a hash table, 
we use a page table to contain the timestamp information. 
To balance runtime overhead and memory overhead, 
we design a four-layer page table for 32-bit programs.
We use 4-KB consecutive memory areas to hold pointers pointing to 
memory areas in the next layer 
or memory areas holding timestamps for memory cells. 
%For a monitored 32-bit program, 
%we need to calculate four addresses using bitwise operations 
%and use the four addresses to refer to each layer of the page table.  
%For 64-bit programs, we use six layers.  
Compared with the hash table, 
the page table leverages the locality of memory accesses 
and can lead 
to a better cache performance. 
%To further improve performance, 
%we add an extra variable to hold the pointer pointing 
%to the last referenced memory area holding timestamps.
%For each memory read or write, 
%we first check whether the saved pointer value can be used. 
%We only conduct a page table lookup when the saved value cannot be used. 


\textbf{Optimization 3}
For this optimization, we apply inline functionality 
provided by LLVM to make all instrumented hook functions inline.

\subsubsection{Cost Metrics}
To count LIs, for every monitored loop, we initialize 
a global counter to be $0$ at the beginning of the program and 
increase the counter by $1$ in the loop header.
To count RIs, for every monitored recursive function, 
we initialize a global counter to be $0$ at the beginning of the program
and increase the counter 
in the entry BB of the recursive function. 
To count executed BBs, a naive method is to create a global counter
and increase the counter value in every BB. 


\textbf{Optimization 4}
To efficiently count executed BBs, 
we apply an algorithm, which was originally designed to 
efficiently count edge events through selectively instrumenting a counter 
on CFG~\cite{event-counting}.
The algorithm has already been proved to be able to 
conduct path profiling efficiently~\cite{peter-ase,path-profiling}. 
To apply the algorithm,
we instrument a local counter \texttt{local\_cost} for each function
and initialize its value to be $0$ at the entry BB. 
We will add the value of \texttt{local\_cost} to a global counter \texttt{cost} 
at the exit BB.
After that, we only need to consider where 
and how to update \texttt{local\_cost} 
within a single function.
We design and implement an intra-procedural control flow analysis
to achieve this.
%Given a single function,
%we add a fake edge from the exit BB to the entry BB 
%to make its CFG strongly connected. 
Since the original algorithm is design to count edge events,
we split each BB into two 
and label the event number to be 1 for each edge connecting a pair of split BBs. 
We label the event number to be 0 for all other edges.
We compute a spanning tree~\cite{spanning} for the new CFG.
Edges not in the spanning tree are called chords.
We apply the depth-first search algorithm proposed by~\citet{event-counting} 
to calculate on which chords we should change 
\texttt{local\_cost} 
and how much we should change.

\subsection{Experimental Evaluation}
\label{sec:inhouse_exp}

\subsubsection{Methodology}
Our current implementation of \Tool focuses on C/C++ programs, 
but we believe that our algorithms are general enough to be extended 
to other programming languages.
We conduct our experiments using a Ubuntu-16.04 machine, 
with i7-7700 CPU and 32-GB memory. 


%\input{section/tab_inhouse1}

\textbf{Benchmarks}
We select complexity problems from two sources. 
The first source is our studied complexity problems, 
which are randomly sampled from representative applications in previous works~\cite{PerfBug,SongOOPSLA2014}.
We successfully reproduce 17 complexity problems. 
Among them, 6 bugs are extracted from original Java or JS programs, 
and re-implemented by using C/C++, 
because our current implementation of \Tool can only work on C/C++ programs.
One reproduced bug is extracted from a very old Mozilla version, 
and we fail to find dependent libraries to build that version. 
We do the re-implementation after we thoroughly understand the complexity problems.
During the re-implementation, we maintain the original algorithms, 
data structures, and caller-callee relationships. 
We fail to reproduce other complexity problems, 
because they depend on very special software or hardware environments.
%which are not available to us. 
We fail to re-implement them, 
because their data structures are too complex to extract.

The second source is from Toddler project~\cite{Alabama}. 
Toddler is a dynamic technique that can detect inefficient loops in Java programs.
In a previous work, \citet{ldoctor} re-implemented all 
21 bugs detected by Toddler and verified by developers using C/C++.
During the re-implementation, the researchers first reproduced all Java bugs, 
then they referenced JDK to re-implement 
common data structures, like ArrayList, HashMap and Vector, 
and finally, they re-implemented the performance bugs according to their root cases. 
We use all the 21 re-implemented bugs in our evaluation.  

In total, we evaluate \Tool on 38 benchmarks. 
All benchmark information is shown in Table~\ref{tab:benchmark_info}.
In general, benchmarks used in our experiments are large. 
Except one benchmark, all other benchmarks are more than 100 lines of code. 
seven benchmarks contain more than one million lines of code. 
We implement a set of static analyzers to count static features for these benchmarks. 
For large benchmarks, 
they could contain more than ten thousand loops, more than one 
thousand array-processing loops or linked-list-processing loops, 
and more than ten thousand recursive functions. 
Clearly, automated algorithmic profiling tools are needed to analyze these code constructs. 
The majority of benchmarks used in our experiments are in $O(N^2)$ complexity. 
We also two benchmarks in $O(2^N)$ complexity, 
and two benchmarks in $O(N)$ complexity. 

\input{section/tab_inhouse}

\textbf{Experimental Settings}
For each benchmark, 
we conduct algorithmic profiling on $10$ program runs with distinct inputs, 
following the previous practice during in-house 
testing and diagnosis~\cite{SongOOPSLA2014,joy.asplos13}.
For all benchmarks used in our experiments,
their bug reports contain at least one bug-triggering input. 
For 14 of them, 
the users explicitly explained how to change input sizes to 
observe the described scaling problems when they reported these bugs. 
We follow the users' 
descriptions to generate new inputs.
For the other benchmarks, 
it is not difficult to figure out how to generate new inputs based 
on the bad inputs in the bug reports. 
For example, when reporting GCC\#46401, 
the user provided a C file, 
containing an expression with thousands of strings as operands. 
We change the number of strings to generate new inputs.
For each benchmark, we choose the input sizes of the $10$ inputs to be 
an arithmetic sequence with the largest input as the input for its bug report 
and the smallest input whose input size is 
only $\frac{1}{10}$ of the largest input.

For each benchmark, we first fit a curve to understand how execution 
time changes with input size. 
We then examine whether fitted functions can explain the observed scaling problem, 
by calculating similarity using Equation~\ref{eq:sim}. 
We consider a similarity value larger than $0.85$ as being able to explain the scaling problem. 
We focus on two fitted functions, one is calculated for main function, 
and the other is calculated for recursion function. 
If a complexity problem is caused by repeated executions of a loop,
its recursion function is the function containing the outer loop.
If a complexity problem is caused by a recursive function, 
its recursion function is the recursive function. 
Otherwise, the complexity problem does not have recursion functions. 


\textbf{Research Questions to Answer}
Our experiments are designed to evaluate whether \Tool can provide the desired profiling coverage 
and profiling accuracy. 
Specifically, we aim to answer the following two questions:

\begin{itemize}

\item {\bf RQ1.} 
Benchmarks used in our evaluation are in different types of complexity and 
are caused by different types of buggy code constructs. 
Can \Tool cover all of them? 
 
\item {\bf RQ2.}
Will \Tool generate many inaccurate profiling results and report many false positives? 

\end{itemize}

\subsubsection{Experimental Results} 
For each benchmark, we conduct experiments using two 
combinations of input and cost metrics, one is with 
RMS+ as input metric and BBs as cost metric, 
and the other is with DSS as input metric and LIs as cost metric. 
The two bugs in $O(2^N)$ complexity are caused by recursive functions.
We also evaluate \Tool on these two bugs 
using RMS+ as input metric and RIs as cost metric. 
All experimental results are shown in Table~\ref{tab:benchmark_info}.


\textbf{Coverage Results}
To answer {\bf RQ1}, 
we examine fitted functions for main and recursion functions 
under different combinations. 
Under the combination of RMS+ and BBs, 
the fitted functions of main can explain 
how the execution time of the whole program scales for all benchmarks, 
as shown by the check-marks 
in Table~\ref{tab:benchmark_info} (the ``main-C'' column).
Two benchmarks are in $O(N)$ complexity, and they do not have recursion functions.
The fitted functions of recursion functions fail in these two cases. 
The fitted functions can successfully 
explain how execution time scales for all other benchmarks (the ``RMS+/Re-C'' column).
The combination of RMS+ and BBs does provide good profiling coverage. 
GCC\#1687 and GCC\#27733 are caused by recursive functions. 
We also evaluate the combination of RMS+ and RIs for these two benchmarks.
The fitted functions of the two recursive functions can also explain how execution time scales, 
but the calculated similarity values are smaller, 
compared with using BBs as cost metric 
(the second value in the ``RMS+/Re-C'' column). 

The combination of DSS and LIs fail to explain how execution time scales for six benchmarks.
The buggy code constructs for these benchmarks 
are not array-processing loops or linked-list-processing loops. 
We cannot get input information 
when analyzing their recursion functions 
(``RMS+/Re-C'' column).  
The fitted functions of recursion functions fail in the two $O(N)$ cases too.
For all other cases, the fitted functions of recursion functions can 
explain how execution time scales. 
The combination of DSS and LIs can effectively 
profile complexity problems caused by 
repeated executions of an array-processing loop 
and a linked-list-processing loop. 


\textbf{Accuracy Results}
To answer {\bf RQ2},
we manually examine all functions with superlinear complexity reported by \Tool.
They can be divided into two categories. 
Some of them are on the call chain between main function and the recursion function.
These functions can also explain how execution time scales with input size. 
We do not consider them as false positives. 
For others (the two ``\# of F.P.'' columns), they do scale superlinearly. 
However, their costs are smaller than the cost of 
the recursion function in the same benchmark. 
They are not the main contributors to the perceived performance degradation. 
We consider these functions as benign false positives, 
since they can easily be filtered out 
by comparing cost with recursion functions. 
In general, \Tool is accurate, and it does not report any true false positives. 
Compare with the two combinations, DSS plus 
LIs will generate fewer benign false positives. 

\textbf{Performance Results}
Although the runtime overhead is not a major concern 
for the in-house version of \Tool, 
smaller overhead can allow developers to run 
more test cases under a given time budget. 
Overhead results are shown in Table~\ref{tab:benchmark_info}. 
Compared with RMS+, DSS will incur a smaller overhead across all benchmarks. 


\subsubsection{Discussion} 
Two design points are particularly useful 
to conduct algorithmic profiling for real-world complexity problems.
They are RMS+ plus executed BBs and DSS plus executed LIs. 
The combination of RMS+ and BBs can cover all cases in our evaluation. 
The combination of DSS and LIs can cover most common cases, 
where a complexity problem is caused by an array-processing loop or a linked-list-processing loop, 
while incurring a much lower runtime overhead compared with the combination of RMS+ and BBs. 

