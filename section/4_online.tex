%\newpage
\section{Production-Run Algorithmic Profiling}
\label{sec:online}

In this section, we discuss our design and 
implementation for the production-run version of \Tool. 
For production-run usage, profiles are collected from the user side.
It is very important to keep the runtime overhead low, since
end users cannot tolerate any observable performance slowdown.
To achieve this requirement,
our design follows several principles. 

First, \textit{study guided}. 
The design of the production-run version of \Tool
is guided by the empirical study in Section~\ref{sec:study}.
We focus on the majority of complexity problems, 
caused either by repeated executions of a loop ($N^k$)
or a recursive function ($2^N$).
We focus on common types of data structures, which are array and linked list.

Second, \textit{focused checking}.
When applying the production-run version of \Tool, 
we expect developers will specify a suspicious loop or a suspicious recursive function
to be monitored. 
\Tool will automatically instrument the specified code construct 
to collect runtime information from the user side in a low overhead. 
The profiling results can help developers better understand the complexity of the code construct 
and its processed workloads in the real world.
\Tool can be used together with performance failure 
diagnosis tools~\cite{SongOOPSLA2014} 
or traditional profilers to
focus on suspicious code constructs leading 
to user-perceived performance failures.

Third, \textit{sampling}.
Instead of recording all dynamic information, 
we apply sampling and record only part of the information. 
We infer information for the whole execution based on the collected samples. 
Sampling can effectively lower the runtime overhead. 


\subsection{Technical Design}
As we discussed in Section~\ref{sec:study}, 
the majority of complexity problems studied are caused 
by repeated executions of a loop or a recursive function. 
Previous works show that sampling code constructs that are executed 
multiple times in one program run can lower the runtime overhead compared to not sampling, 
while still being able to collect enough runtime information 
without hurting diagnosis latency~\cite{SongOOPSLA2014,ldoctor}. 
Inspired by our study and the earlier works, 
we apply sampling to efficiently profile loops 
and recursive functions with multiple executions. 

\noindent\textbf{Input Metric}
If the specified code construct is an array-processing loop 
or a linked-list-processing loop,
we will use DSS as the input metric. 
Otherwise, we will use RMS+ as the input metric. 

As we discussed in Section~\ref{sec:inhouse}, 
there are two methods, top-down and bottom-up, 
to analyze RMS+ and DSS records collected 
for multiple dynamic instances of a code construct in one program run. 
We leverage the top-down method for the in-house version of \Tool. 
However, the top-down method is not suitable for sampling. 
The reason is as follows.
Assume we have a code construct \texttt{c} to monitor. 
It is inside a loop \texttt{l} and is executed multiple times in one program run.
There are fewer dynamic instances of \texttt{l}
than dynamic instances of \texttt{c}.
If we apply the top-down method, 
we need to sample dynamic instances of \texttt{l}, 
and it is very likely that we will miss these instances. 
For each dynamic instance of \texttt{l}, 
there will be more computation, 
compared with an instance of \texttt{c}.
More overhead will be incurred to collect 
information for dynamic instances of \texttt{l}.
Therefore, we apply the bottom-up method 
in the production-run version of \Tool.
We sample instances of \texttt{c} to record 
distinct memory cells contributing RMS+ 
or distinct elements in an array or a linked list.
We use the sampled information to infer RMS+ 
or DSS for all instances of \texttt{c} in one program run.


The sampling method is similar to that described in previous works on statistical 
debugging~\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}.
We make a cloned version of the monitored code construct.
We instrument the cloned version to record information for RMS+ or DSS. 
We dump the recorded information to log 
whenever the cloned version finishes execution. 
We add extra delimiters to log to differentiate information collected from different instances.
Before each execution of the monitored code construct, 
we choose between the cloned version and the original version. 
How many times the cloned version is executed 
depends on a tunable sampling rate. 
To make the choice between the two versions,
we add a global counter to the monitored program. 
If the counter value is larger than $0$, 
we choose the original version and decrease the counter value by $1$.
If the counter value is equal to $0$,
we choose the cloned version and reset the counter value to 
a random number, 
whose expectation is equal to the inverse of the sampling rate.  


We leverage the mark-and-recapture method~\citep{mark-recapture} to 
estimate RMS+ or DSS for all dynamic instances of a code construct 
based on the collected samples. 
Mark-and-recapture is a commonly used statistical method 
for estimating the size of an animal population. 
In this method, some of the animals are captured, marked, and released. 
Then, another group of the animials are captured.
The size of the whole animal population is estimated 
based on the ratio of marked animals in the second captured sample.  


Each sample of a monitored code construct is a set of memory cells, 
which contribute RMS+ or represent distinct elements in an array or a linked list. 
In one program run, we assume that we collect a sequence of $m$ samples. 
Given the $i$th sample, $M_i$ represents the 
total number of distinct memory cells in the previous $i-1$ samples, 
$C_i$ represents the number of distinct memory cells in the $i$th sample,
and $R_i$ represents the number of distinct memory cells in 
the $i$th sample that also appeared in one of the previous $i-1$ samples.
The total number of distinct memory cells for all dynamic instances 
of the monitored code construct can be estimated as:


%\begin{equation} \label{eq:mark}
%$$p_v(S_v(a)) = 1 - \prod\limits_{u \in S_v(a)}(1 - p_{u,v})$$
%N = \frac{\sum\limits_{i=1}^m M_i*C_i}{\sum\limits_{i=1}^m R_i}
%\end{equation}

\begin{equation} \label{eq:mark}
N = \sum\limits_{i=1}^m M_i*C_i\Big/\sum\limits_{i=1}^m R_i
\end{equation}

\noindent\textbf{Cost Metric}
The production-run version of \Tool focuses on recursive functions or loops.
If the monitored code construct is a recursive function,
we use RIs as the cost metric.
If the monitored code construct is a loop, 
we use LIs as the cost metric. 
We do not apply sampling when collecting these two metrics. 
As we discussed in Section~\ref{sec:inhouse},
these two metrics will incur a smaller overhead compared with BBs, 
and they can still provide accurate profiling results.  


\subsection{Experimental Evaluation}
%\subsubsection{Research Questions}

\subsubsection{Methodology}
We will conduct experiments to answer the following two research questions:

\begin{itemize}
\item {\bf RQ1.} 
Can sampling lower the runtime overhead while giving the same profiling results? 
A positive answer means the effectiveness of the production-run version of \Tool. 

\item {\bf RQ2.} 
Will sampling increase profiling latency? 
By applying sampling, less information is collected in one single run. 
If we have to monitor more program runs to get the same profiling results,
the profiling latency is increased.
As we discussed earlier, the majority of complexity problems are caused 
by repeated executions of a loop or a recursive function.
It is possible that sampling can still collect enough information in one program run, 
while not increasing the profiling latency. 


\end{itemize}

\noindent\textbf{Benchmarks and Inputs}
We reuse all benchmarks in Table~\ref{tab:benchmark_info}.
We monitor the loop or the recursive function with the most iterations\footnote{We consider a recursive function call instance as a loop iteration.} 
for all benchmarks,
except for Mozilla\#347306 and GCC\#46401. 
%These two bugs are caused repeated execution of a loop, 
%whose total iteration numbers ranked 2nd. 
%Instead, we monitor these two loops.
These two bugs are caused by the repeated executions of a loop with the second
highest number of iterations. Instead of using the loop with the most iterations,
we monitor these two loops.


For each benchmark, 
we use the same methodology described in Section~\ref{sec:inhouse_exp} 
to generate a sequence of inputs, 
so that the difference between the sizes of two consecutive inputs is constant.
This input set contains controlled inputs.
We also randomly sample controlled inputs to generate a set of random 
inputs to emulate the uncontrolled inputs in production runs.    

\noindent\textbf{Metrics}
We will measure the following three metrics during our experiments:
1) runtime overhead, which is the slowdown caused 
by sampling information from each program run;
2) profiling capability, which is measured as the similarity between two cost functions 
inferred under the in-house setting and the production-run setting using the same metrics;
and 3) profiling latency, which is measured by how many program 
runs are needed for the algorithmic profiling. 


\input{section/tab_online1}

\input{section/tab_online2}

\noindent\textbf{Settings}
By default, we set the sampling rate to be $1$ out of $100$ 
and run the algorithmic profiling using $100$ program runs with controlled inputs.  

Besides the default setting,
we evaluate the impact of different numbers of monitored program runs, 
ranging from $10$ to $1000$, under the default sampling rate.
We conduct this experiment by using both controlled inputs and random inputs 
to understand whether different input sets will influence the experimental results.
We also evaluate the impact of changing the sampling rate, ranging from $1$ out of $10$ to $1$ out of $10000$, 
using $100$ program runs with controlled inputs.  

Sampling will introduce some randomness. 
We conduct each experiment multiple times, 
and our results are stable across multiple experiments. 
In particular, overhead is measured using $10$ runs under the same setting. 

\noindent\textbf{Runtime Overhead}
Table~\ref{tab:overhead} shows that the runtime overhead is small under the 
default sampling rate (1 out of 100).
It is below $5\%$ in $34$ out of $38$ cases, 
and it is below $10\%$ in $35$ out of $38$ cases. 
It is below $5\%$ for all real complexity problems, 
except GCC\#1687. 

The runtime overhead is influenced by the sampling rate, 
as shown in Figure~\ref{tab:sampling}.
We can lower the runtime overhead by decreasing the sampling rate.
The overhead can be lowered to be mostly ($30/38$) 
under $1\%$ when the sampling rate is $\frac{1}{10^4}$.
The runtime overhead increases, 
if we sample more frequently.
The runtime overhead will be more than $10\%$ for most cases ($30/38$), 
when the sampling rate is $\frac{1}{10}$.

\noindent\textbf{Profiling Capability}
As shown in Table~\ref{tab:overhead}, 
with $1000$ program runs and controlled inputs, 
sampling does very little damage to the profiling capability. 
We fail to profile Mozilla\#490742 and Apache\#37184.
These two bugs are in $O(N)$ complexity. 
Their monitored loops execute only once 
and we fail to collect any sample.
For all other bugs, 
%the calculated similarity by using Equation~\ref{eq:sim}
%for the two inferred functions under 
%in-house and production-run setting
%is constantly larger than $0.85$.
the similarity calculated with Equation~\ref{eq:sim} for the two functions with the same input and cost metrics
under in-house setting and production-run setting is consistently larger than $0.85$.

As expected, the profiling capability will decrease when using sparser sampling rate. 
As shown in Table~\ref{tab:sampling}, 
when using the default 100 program runs,
the profiling capability is the same for the $\frac{1}{10}$ sampling rate 
and the $\frac{1}{10^2}$ sample rate.
The profiling capability will drop with the $\frac{1}{10^3}$ sampling rate. 
Nine benchmarks that can be accurately profiled with a higher sampling rate
cannot be profiled with the $\frac{1}{10^3}$ sampling rate.
The profiling capability drops further with the $\frac{1}{10^4}$ sampling rate. 
Only six benchmarks can be accurately profiled with the $\frac{1}{10^4}$ sampling rate. 
More program runs are needed for the $\frac{1}{10^3}$ sampling rate 
and the $\frac{1}{10^4}$ sampling rate. 

As shown in Table~\ref{tab:sampling}, 
using different inputs has a slight impact on the profiling capability. 
For $500$ and $1000$ program runs, 
the profiling capability is the same between controlled inputs and random inputs. 
For $100$ program runs, 
the profiling capability is better when using controlled inputs.
Two benchmarks that can be accurately profiled with controlled inputs 
cannot be accurately profiled with random inputs.
For $10$ program runs, 
some benchmarks can be accurately 
profiled with one input set, but they cannot 
be accurately profiled with the other, 
for both controlled inputs and random inputs.

\noindent\textbf{Profiling Latency}
Table~\ref{tab:sampling} shows the quantitative measurements for the impact of sampling on the profiling latency.
As we can see, with controlled inputs, 
three benchmarks need around 100 program runs for an accurate profile. 
Mozilla\#490742 and Apache\#37184 need more than 1000 runs for an accurate profile to be produced. 
With random inputs, two benchmarks, GCC\#1687 and GCC\#27733, 
need around 100 runs for an accurate profile to be produced. 
Apache\#29743 needs around 500 runs. 
Mozilla\#490742 and Apache\#37184 also need more than 1000 runs.   
This means longer profiling latencies than the in-house version of \Tool,
which needs only 10 runs to for an accurate profile. 

Sampling does not lengthen the profiling latency for 32 benchmarks 
with controlled 
inputs and for 33 benchmarks with random inputs.
As we discussed earlier, these bugs are caused by repeated executions of a loop. 
Even though we apply sampling, 
we can still collect enough information to make an accurate estimate for the whole execution. 
Consequently, sampling allows us to achieve a low runtime overhead, 
accurate profiling results, and a low profiling latency at the same time. 
For other benchmarks, even though we use the $\frac{1}{10^2}$ sample rate, 
sampling does not lengthen the profiling latency by 100 times. 
For most benchmarks, 100 runs are enough for an accurate profile, 
since the root-cause loop or recursive function is executed many times, 


















%\input{section/tab_online1}
%\input{section/tab_online2}




%\subsection{Discussions}

