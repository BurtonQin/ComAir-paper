\section{Related Works}
\label{sec:related}

{\underline{\textit{Empirical Studies on Performance Problems.}}
Many characteristics studies are conducted on real-world performance 
problems~\cite{PerfBug,SongOOPSLA2014,ldoctor,Zaman2012MSR,Nistor2013MSR,HuangRegression,SmartphoneStudy,junwen-1, junwen-2}. 
~\citet{PerfBug} collect 110 real-world performance bugs from 5 representative applications.
Their study tries to understand root causes of performance problems,
how performance problems are introduced, how to expose them, and how to fix them.
In their following works, they focus their studies on 
user-perceived performance problems~\cite{SongOOPSLA2014} 
and real-world inefficient loops~\cite{ldoctor}. 
~\citet{Zaman2012MSR} conduct qualitative comparison between performance bugs 
and non-performance bugs collected from FireFox and Chrome. 
~\citet{Nistor2013MSR} compare performance bugs with non-performance bugs 
from aspects of discovering, reporting, and fixing. 
~\citet{HuangRegression} try to figure out what code changes are more likely to introduce performance regressions through 
studying 100 performance regressions. 
~\citet{SmartphoneStudy} find common inefficiency patterns after 
inspecting 70 performance problems from smartphone applications. 
There are also research work focusing on inefficiency in web applications~\cite{junwen-1, junwen-2}. 
Similar to our work in Section~\ref{sec:study}, 
these studies have important findings and can guide tool design to combat 
real-world performance problems. 
However, our work is the first empirical study on real-world complexity problems, 
and it provides important supplement to existing studies. 

{\underline{\textit{Profilers.}}
Profiler is the most widely used tool during performance optimization and performance debugging. 
After collecting runtime information, 
profilers wil associate performance metrics to executed instructions, 
functions, or calling context~\cite{oprofile,gprof, CCT}.
Many research works are proposed to improve 
accuracy of profilers~\cite{4Profilers, LagHunter, AppInsight} or 
to reduce runtime~\cite{AdaptiveBurst} or memory overhead of profilers~\cite{HotCallingContext}. 
As we discussed earlier, profilers can only analyze one single execution, 
but fail to connect analysis results from different executions 
or to predict analysis results for inputs not seen before.  


Both aprof~\cite{Aprof1, Aprof2} and AlgoProf~\cite{AlgoProf} try to 
associate complexity to code constructs. 
There are three differences between aprof and AlogProf. 
First, aprof attributes complexity to functions, 
while AlgoProf analyzes complexity for loops and recursive functions. 
Second, aprof measure input size as the number distinct memory cells 
first accessed read operations, but AlgoProf consider input size as the size of data structures. 
Third, aprof use the number of executed basic blocks as cost metric, 
while AlgoProf defines a set of primitive operations, like loop iteration, structure read, 
and structure creation, and use the number of primitive operations as cost metric. 
We compared different design points for in-house algorithmic profiling in Section~\ref{sec:inhouse}. 
As we mentioned earlier, both aprof and AlgoProf can introduce large runtime overhead, 
and are not suitable to apply in production runs. 

